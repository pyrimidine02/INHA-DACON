{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:17.760380Z",
     "start_time": "2024-07-04T10:04:17.538306Z"
    }
   },
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/sonhoyoung/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:17.764840Z",
     "start_time": "2024-07-04T10:04:17.762250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "import re"
   ],
   "id": "d6747031fd8ec34e",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:17.768347Z",
     "start_time": "2024-07-04T10:04:17.765851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasetName = \"train.csv\"\n",
    "jsonFileName = \"indata_kor.jsonl\""
   ],
   "id": "cc9c64018c80367b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:19.679587Z",
     "start_time": "2024-07-04T10:04:17.770255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_whitespace(text):\n",
    "    # \\n을 공백으로 변환하고, 두 칸 이상의 공백을 한 칸의 공백으로 변환합니다.\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, encoding='euc-kr')  # 인코딩을 명시적으로 지정\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')  # 다른 인코딩을 시도\n",
    "    \n",
    "    with open(json_file_path, \"w\", encoding='utf-8') as json_file:\n",
    "        for index, row in df.iterrows():\n",
    "            data = {\n",
    "                'context': normalize_whitespace(row['context']),\n",
    "                'instruction': normalize_whitespace(row['question']),\n",
    "                'response': normalize_whitespace(row['answer'])\n",
    "            }\n",
    "            json.dump(data, json_file, ensure_ascii=False)\n",
    "            json_file.write(\"\\n\")\n",
    "\n",
    "dataPath = './data/'\n",
    "datasetName = \"train.csv\"\n",
    "jsonFileName = \"INHA-DACON.jsonl\"\n",
    "csv_file_path = dataPath + datasetName\n",
    "json_file_path = dataPath + jsonFileName\n",
    "\n",
    "csv_to_json(csv_file_path, json_file_path)\n"
   ],
   "id": "8dbb0e0239db0e24",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:20.262823Z",
     "start_time": "2024-07-04T10:04:19.680268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "\n",
    "# JSONLines 파일을 읽어서 데이터셋 생성\n",
    "indataset = []\n",
    "with jsonlines.open(json_file_path) as f:\n",
    "    for line in f.iter():\n",
    "        context = line['context']\n",
    "        instruction = line['instruction']\n",
    "        response = line['response']\n",
    "        indataset.append(f\"<s>[INST] {instruction} [/INST] {context} [/INST] {response}</s>\")\n",
    "\n",
    "print('데이터셋 확인')\n",
    "print(indataset[:5])\n",
    "\n",
    "# 데이터셋을 사전으로 변환하여 Hugging Face Dataset으로 저장\n",
    "indataset = Dataset.from_dict({'text': indataset})\n",
    "indataset.save_to_disk(dataPath)\n",
    "\n",
    "print('데이터셋 info 확인')\n",
    "print(indataset)"
   ],
   "id": "508d9e9633fa2b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 확인\n",
      "['<s>[INST] 실적 부진으로 인해 르노그룹에서 자구책을 어디서 세웠어 [/INST]  르노그룹이 지분 약 80%를 보유한 자회사 르노삼성자동차의 경영실적 부진에 대해 비용 절감 등 고강도 자구책을 주문하고 나섰다. 17일 르노그룹에 따르면 지난 14일(현지시간) 프랑스 본사에서 그룹 경쟁력 강화를 위해 수익성을 중심으로 경영 전략을 전환하는 그룹의 새로운 경영전략안인 ‘르놀루션’을 발표했다. 그룹이 이 같은 경영전략안을 내놓게 된 것은 코로나19 여파로 인해 메르세데스-벤츠, BMW 등 글로벌 자동차 업체들이 판매부진을 겪고 있는 상황에서 르노그룹도 수조 원의 적자를 내는 등 경영에 어려움을 겪고 있는 것과 무관치 않다. 이날 내놓은 경영전략안에는 수익성 확보를 위한 다양한 방안과 함께 그룹 산하 조직별로 경쟁력과 비용 등을 책임진다는 내용이 포함됐다. 특히 르노삼성차가 있는 한국을 포함해 인도, 라틴아메리카를 현재보다 수익성을 더욱 강화해야 할 국가로 지목하면서 부산공장에 미칠 파장이 적지 않을 것으로 보인다. 르노삼성차는 지난해 수출이 77.7% 급감하면서 700억 원 안팎의 적자를 낸 것으로 알려졌다. 이는 8년 만의 적자로, 2014년 이후 부산공장에서 생산되며 수출 효자 노릇을 한 닛산 ‘로그’가 지난해 3월부터 생산라인에서 빠진 것이 영향을 미쳤다. 이 같은 실적 부진과 관련, 르노삼성차 측은 최근 전체 임원을 40%가량 줄이고 급여도 20% 삭감하기로 했다. 향후 비용 절감을 위한 구조조정도 불가피할 것으로 보여 노조와의 충돌도 예상된다. 르노삼성차 관계자는 “2011~2012년 대규모 적자에도 신차 개발 등으로 회생을 한 만큼 이번에도 잘 극복해 나갈 것”이라고 말했다. [/INST] 르노삼성자동차</s>', '<s>[INST] 실적 부진으로 인해 르노그룹에서 자구책을 세운 대상은 어디야 [/INST]  르노그룹이 지분 약 80%를 보유한 자회사 르노삼성자동차의 경영실적 부진에 대해 비용 절감 등 고강도 자구책을 주문하고 나섰다. 17일 르노그룹에 따르면 지난 14일(현지시간) 프랑스 본사에서 그룹 경쟁력 강화를 위해 수익성을 중심으로 경영 전략을 전환하는 그룹의 새로운 경영전략안인 ‘르놀루션’을 발표했다. 그룹이 이 같은 경영전략안을 내놓게 된 것은 코로나19 여파로 인해 메르세데스-벤츠, BMW 등 글로벌 자동차 업체들이 판매부진을 겪고 있는 상황에서 르노그룹도 수조 원의 적자를 내는 등 경영에 어려움을 겪고 있는 것과 무관치 않다. 이날 내놓은 경영전략안에는 수익성 확보를 위한 다양한 방안과 함께 그룹 산하 조직별로 경쟁력과 비용 등을 책임진다는 내용이 포함됐다. 특히 르노삼성차가 있는 한국을 포함해 인도, 라틴아메리카를 현재보다 수익성을 더욱 강화해야 할 국가로 지목하면서 부산공장에 미칠 파장이 적지 않을 것으로 보인다. 르노삼성차는 지난해 수출이 77.7% 급감하면서 700억 원 안팎의 적자를 낸 것으로 알려졌다. 이는 8년 만의 적자로, 2014년 이후 부산공장에서 생산되며 수출 효자 노릇을 한 닛산 ‘로그’가 지난해 3월부터 생산라인에서 빠진 것이 영향을 미쳤다. 이 같은 실적 부진과 관련, 르노삼성차 측은 최근 전체 임원을 40%가량 줄이고 급여도 20% 삭감하기로 했다. 향후 비용 절감을 위한 구조조정도 불가피할 것으로 보여 노조와의 충돌도 예상된다. 르노삼성차 관계자는 “2011~2012년 대규모 적자에도 신차 개발 등으로 회생을 한 만큼 이번에도 잘 극복해 나갈 것”이라고 말했다. [/INST] 르노삼성자동차</s>', '<s>[INST] 사원들에게 신년사를 이메일로 전달한 사람은 누구야 [/INST]  대한항공과 아시아나항공이 신년사를 통해 올해 ‘통합’에 전념하겠다는 뜻을 밝혔다. 조원태 한진그룹 회장은 4일 임직원 이메일로 배포한 영상 신년사에서 “대한항공과 아시아나항공 통합은 양사 임직원에게 주어진 운명이자 시대적 사명이라고 믿는다”고 말했다. 조 회장은 “하늘 아래 양사 임직원은 모두 하나다. 입고 있는 옷과 서 있는 자리만이 달랐을 뿐 고객을 섬기는 자세와 나라를 생각하는 마음은 다르지 않았다”며 “이해와 공감을 바탕으로 서로의 모자란 부분을 채워주고 보듬어주면 좋겠다”고 당부했다. 이어 “인수를 바라보는 많은 분들의 우려도 충분히 이해한다”며 “우리가 무엇을 해낼 수 있는지 알려면 우리는 도전해야 한다”고 강조했다. 조 회장은 “항공사 통합으로 글로벌 항공 역사에 길이 남을 우리만의 이야기를 함께 만들어나가기를 진심으로 기대한다”면서 “많이 어렵고 힘들겠지만, 힘을 모아야 한다. 거기에서부터 하나가 된 우리의 새로운 이야기가 시작될 것”이라고 역설했다. 정성권 아시아나항공 대표이사(내정)도 이날 사내 게시판에 올린 신년사에서 “대한항공과의 통합과정에서 양사가 최대의 시너지를 발휘하는 발판을 마련하겠다”고 밝혔다. 정 대표는 “지난달부터 통합 계획 수립을 위한 실사가 시작됐다”며 “이달에는 우리나라를 비롯해 양사가 취항하는 주요 국가에 기업결합신고를 하고, 인수 절차는 상반기 말 최종 마무리될 예정”이라고 말했다. 통합 이후 고용 보장에 대해서는 “산업은행과 대한항공에서 인위적인 구조조정은 없을 것임을 여러 차례 밝힌 바 있다”며 “인수 절차 완료 후에도 우리 임직원이 각자의 능력을 충분히 발휘하고 아시아나 강점이 조직 발전의 원동력이 될 수 있도록 토대를 닦는 데 모든 노력을 다하겠다”고 강조했다. [/INST] 조원태</s>', '<s>[INST] 누가 사원들에게 이메일을 통해 신년사를 보냈어 [/INST]  대한항공과 아시아나항공이 신년사를 통해 올해 ‘통합’에 전념하겠다는 뜻을 밝혔다. 조원태 한진그룹 회장은 4일 임직원 이메일로 배포한 영상 신년사에서 “대한항공과 아시아나항공 통합은 양사 임직원에게 주어진 운명이자 시대적 사명이라고 믿는다”고 말했다. 조 회장은 “하늘 아래 양사 임직원은 모두 하나다. 입고 있는 옷과 서 있는 자리만이 달랐을 뿐 고객을 섬기는 자세와 나라를 생각하는 마음은 다르지 않았다”며 “이해와 공감을 바탕으로 서로의 모자란 부분을 채워주고 보듬어주면 좋겠다”고 당부했다. 이어 “인수를 바라보는 많은 분들의 우려도 충분히 이해한다”며 “우리가 무엇을 해낼 수 있는지 알려면 우리는 도전해야 한다”고 강조했다. 조 회장은 “항공사 통합으로 글로벌 항공 역사에 길이 남을 우리만의 이야기를 함께 만들어나가기를 진심으로 기대한다”면서 “많이 어렵고 힘들겠지만, 힘을 모아야 한다. 거기에서부터 하나가 된 우리의 새로운 이야기가 시작될 것”이라고 역설했다. 정성권 아시아나항공 대표이사(내정)도 이날 사내 게시판에 올린 신년사에서 “대한항공과의 통합과정에서 양사가 최대의 시너지를 발휘하는 발판을 마련하겠다”고 밝혔다. 정 대표는 “지난달부터 통합 계획 수립을 위한 실사가 시작됐다”며 “이달에는 우리나라를 비롯해 양사가 취항하는 주요 국가에 기업결합신고를 하고, 인수 절차는 상반기 말 최종 마무리될 예정”이라고 말했다. 통합 이후 고용 보장에 대해서는 “산업은행과 대한항공에서 인위적인 구조조정은 없을 것임을 여러 차례 밝힌 바 있다”며 “인수 절차 완료 후에도 우리 임직원이 각자의 능력을 충분히 발휘하고 아시아나 강점이 조직 발전의 원동력이 될 수 있도록 토대를 닦는 데 모든 노력을 다하겠다”고 강조했다. [/INST] 조원태</s>', '<s>[INST] PM9A3 E1.S의 연속쓰기 속도는 [/INST]  삼성전자가 OCP(오픈 컴퓨트 프로젝트)의 규격을 만족하는 데이터센터 전용 고성능 SSD ‘PM9A3 E1.S’를 양산한다고 24일 밝혔다. OCP는 글로벌 데이터센터 관련 기업들이 효율적인 데이터센터 개발과 운영에 필요한 하드웨어와 소프트웨어의 표준을 정립하는 기구다. 이번 제품은 업계최초 6세대 V낸드를 기반으로 한 데이터센터 전용 SSD로, OCP의 NVMe Cloud SSD 표준을 지원하며, 데이터센터에서 요구하는 성능, 전력 효율, 보안 등을 각각 최고 수준의 솔루션으로 제공한다. 특히 전력 효율이 업계 최고 수준으로 높아 데이터센터 운영 비용을 절감할 수 있으며, 최근 화두가 되고 있는 탄소 저감 효과도 기대할 수 있다. PM9A3 E1.S의 전력 효율은 연속쓰기 성능을 기준으로 할 때 1와트(W)당 283MB/s를 지원하며, 이는 이전 세대인 5세대 V낸드 기반 PM983a M.2 보다 약 50% 향상됐다. 지난해 전세계 서버용으로 출하된 HDD(하드 디스크 드라이브)를 모두 PM9A3 E1.S 4TB로 대체하면 1년간 절감할 수 있는 전력량이 1484GWh에 이른다. PM9A3 E1.S의 연속쓰기 속도는 3000MB/s로 이전 세대인 제품 대비 연속 쓰기 속도가 약 배 증가했으며, 임의읽기 속도(750K IOPS)와 임의쓰기 속도(160K IOPS)도 각각 40%, 150% 향상됐다. 이번 제품은 사용자 데이터 암호화와 같은 기본적인 보안 기능 뿐만 아니라 안티롤백, 보안 부팅 등 다양한 보안 솔루션을 제공한다. 안티롤백은 보안이 취약한 하위 버전의 펌웨어가 다운로드 되지 못하도록 막는 기능으로, PM9A3 E1.S는 보안 취약점이 발견된 펌웨어에 대해서는 이력을 따로 저장해놓고 해당 버전을 다운로드할 경우 정상적으로 처리되지 않도록 막는다. 이번 SSD는 기본적으로 허가되지 않은 접근이 안되도록 설계돼 있지만, 보안 부팅 기능을 추가해 보안을 더욱 강화했다. 보안 부팅은 SSD 내부에 갖고 있는 전자서명을 부팅 과정에서 체크해, 정상적으로 인식되는 경우에만 부팅이 될 수 있도록 한다. 삼성전자 메모리사업부 상품기획팀 박철민 상무는 \"PM9A3 E1.S는 6세대 V낸드 기반으로 업계 최고 수준의 전력 효율을 구현한 NVMe SSD로 대규모 데이터센터 고객들에게 최적의 솔루션이 될 것\"이라며, \"향후 OCP에 참여한 다양한 고객사들과 협력해 데이터센터용 SSD 표준을 만들어 나갈 것\"이라고 밝혔다. 페이스북의 OCP SSD 총괄 로스 스텐포트는 “OCP NVMe 클라우드 SSD는 최근 데이터센터의 기술적 난제를 해결하는데 매우 중요한 실마리를 제시해준다”면서 “특히 삼성전자의 이번 제품은 대규모 확장이 필요한 데이터센터 환경에 적합한 SSD 요구 사양을 충족하고 있다”고 밝혔다. 삼성전자는 이번 PM9A3 E1.S 양산을 시작으로 5G 시대의 본격 개막과 초연결 라이프 스타일로 변화하는 시대에 발맞춰 글로벌 데이터센터 업체들과 지속 협력하며 차세대 기술 확보와 표준화를 주도해 나갈 계획이다. [/INST] 3000MB/s</s>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33716 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cc04504b190403fb7375b0f3de393c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 info 확인\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 33716\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T10:04:26.090986Z",
     "start_time": "2024-07-04T10:04:20.263720Z"
    }
   },
   "cell_type": "code",
   "source": "indataset.push_to_hub(\"sonhy02/INHA-DACON\")",
   "id": "5ef3c3d10d15b84f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18b8f8aa033c41c58e31362465aeb20b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/34 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2b847b928bb4d1ea00eec5fa9483d84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/278 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6add8b4b4551421a82646647820ce899"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sonhy02/INHA-DACON/commit/2822ced4a86f59141b33ad8ff95bff3e58ea43d9', commit_message='Upload dataset', commit_description='', oid='2822ced4a86f59141b33ad8ff95bff3e58ea43d9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

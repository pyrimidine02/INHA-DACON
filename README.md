# 2024 INHA DACON

---
[**2024 ì¸í•˜ ì¸ê³µì§€ëŠ¥ ì±Œë¦°ì§€**](https://dacon.io/competitions/official/236291/overview/description)
---
---
## **1. Structure of the file**

```
â”œâ”€â”€ CSV_to_jsonl.ipynb               # Convert CSV to JSONL
â”œâ”€â”€ Lora.ipynb                       # Fine-tuning the model(Not Used!)
â”œâ”€â”€ Lora_v2.ipynb                    # Fine-tuning the model(Recent)
â”œâ”€â”€ Modelfile                        # ollama custom model setting(Not Used!)
â”œâ”€â”€ README.md
â”œâ”€â”€ baseline.ipynb                   # Base line(Not Used!)
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ INHA-DACON.jsonl
â”‚Â Â  â”œâ”€â”€ dataset_info.json
â”‚Â Â  â”œâ”€â”€ sample_submission.csv
â”‚Â Â  â”œâ”€â”€ state.json
â”‚Â Â  â”œâ”€â”€ test.csv
â”‚Â Â  â””â”€â”€ train.csv
â”œâ”€â”€ main.ipynb                      # Make submission.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ result
    â””â”€â”€ submission_0705.csv         # Best score

```
---
## **2. Install**

###  **a. Libraries**


The standard of the python version is 3.10
```
$ pip3 install -r requirements.txt
```
###  **b. Models and Lora**
Use Huggingface ğŸ¤— Transformers to download Models and Lora
```
sonhy02/INHA-DACON-2024-8B-4BIT     #Quantizationed Model
sonhy02/INHA-DACON-2024-Lora        #Lora
```
---

## **3. LLM Model**

###  **a. About the Models**
- [~~llama3 8B~~](https://ollama.com/library/llama3:8b)  (Not Used!)
- [~~EEVE-Korean-Instruct-10.8B-v1.0-GGUF~~](https://huggingface.co/heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF/tree/main) (Not Used!)
- [~~beomi/Llama-3-Open-Ko-8B~~](https://huggingface.co/beomi/Llama-3-Open-Ko-8B)  (Not Used!)
- [MLP-KTLim/llama-3-Korean-Bllossom-8B](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B)
- [sonhy02/INHA-DACON-2024-8B-4BIT](https://huggingface.co/sonhy02/INHA-DACON-2024-8B-4BIT)

###  **b. How to use**
Change the `model_id` to your fine-tuned model's path.
```python
# load the model.
model_id = "./models/20240703" # <-- CHANGE HERE TO YOUR MODEL PATH
model = AutoModelForCausalLM.from_pretrained(model_id,
                                            torch_dtype="auto", load_in_4bit=True)
```
---

## **3. History**

### 2024.07.05.
**Lora**
```python
lora_config = LoraConfig(
     task_type=TaskType.CAUSAL_LM,
     r=1,
     target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],
     lora_alpha = 2,
     lora_dropout=0.05,
     #modules_to_save=['embed_tokens','lm_head']
)

training_args = TrainingArguments(
    # torch_compile = True,
    output_dir = './results',
    num_train_epochs = 1,
    fp16=True,
    per_device_train_batch_size=1,
    #per_device_eval_batch_size=1,
    gradient_accumulation_steps=5,
    save_strategy='epoch',
    #evaluation_strategy='epoch',
    save_total_limit=1,
    optim='adamw_bnb_8bit',
    #load_best_model_at_end=True,
    save_only_model=True,
    logging_strategy='steps',
    logging_steps=100,
    label_names=['labels'],
    report_to='tensorboard',
)
```
**Prompt**
```python
question_prompt = f"ë„ˆëŠ” ì£¼ì–´ì§„ Contextë¥¼ í† ëŒ€ë¡œ Questionì— ë‹µí•˜ëŠ” ì±—ë´‡ì´ì•¼. \
                                Questionì— ëŒ€í•œ ë‹µë³€ë§Œ ê°€ê¸‰ì  í•œ ë‹¨ì–´ë¡œ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ë„ë¡ í•´. \
                                Context: {context} Question: {question}\n Answer:"
```


**Result**
```
Answer for question: ê°•ì›ë„ ì „ ì§€ì—­ì—ì„œ 91%ì˜ ìˆ˜ì¶œì„ ì í•œ ì§€ì—­ì´ ì–´ë””ì•¼ : ì›ì£¼ í™ì²œ ë™í•´ ì¶˜ì²œ ê°•ë¦‰
Processed count: 1
Answer for question: ì „í•­ì¼ ì‚¬ì¥ì´ ì´ë² ì´ì¬íŒ¬ ëŒ€í‘œë¡œ ì·¨ì„í•œ ì—°ë„ : 2018ë…„
Processed count: 2
Answer for question: ìƒ¤ë¥´ë¥´ì˜ í¸ê³¡ì— ì°¸ê°€í•œ ì‚¬ëŒì´ ëˆ„êµ¬ì•¼ : doko ë„ì½”
Processed count: 3
Answer for question: ì˜ˆë¹„ì‚¬íšŒì ê¸°ì—… ì¼ìë¦¬ì°½ì¶œì‚¬ì—… ê³µëª¨ì— ì‹ ì²­í•œ ê¸°ì—…ì˜ ìˆ˜ : 52ê°œ ê¸°ì—…
Processed count: 4
Answer for question: ì–´ë””ì„œ ìš°ìˆ˜ ì²­ë…„ ì°½ì—…ìë¥¼ ë°œêµ´í•´ ë‹¤ì–‘í•œ ì§€ì›ì„ í•´ì£¼ëŠ” ì‚¬ì—…ì„ ì‹œí–‰í•´ : ì²­ë…„ì°½ì—…ì‚¬ê´€í•™êµ
Processed count: 5
Answer for question: ì–´ë””ì—ì„œ êµ­ë‚´ ìš°ì£¼ê°œë°œ ì •ì±… ìˆ˜ë¦½ì˜ ê±¸ë¦¼ëŒì´ ë˜ëŠ” ì¶”ê²©í˜• ì—°êµ¬ë¥¼ íƒˆí”¼í•œ ìƒˆë¡œìš´ ì—°êµ¬ë¥¼ í•˜ëŠ” ê±°ì•¼ : í•œêµ­í•­ê³µìš°ì£¼ì—°êµ¬ì›
Processed count: 6
Answer for question: í•œêµ­ìƒˆë†ë¯¼ì¶©ë¶ë„íšŒê°€ ì·¨ì•½ê³„ì¸µì˜ ì‚¬ëŒë“¤ì„ ìœ„í•´ ì¶©ë¶ì‚¬íšŒë³µì§€ê³µë™ëª¨ê¸ˆíšŒì— ì „ë‹¬í•œ ê±´ ë­ì•¼ : ë†ì‚°ë¬¼ ê¾¸ëŸ¬ë¯¸ 200ìƒì
Processed count: 7
Answer for question: ì–´ë–¤ ì‚¬ëŒë“¤ì—ê²Œ ê¸ˆê°ì›ì—ì„œ ë§ì¶¤í˜• ì§€ì›ì„ í•œë‹¤ëŠ” ê±°ì•¼ : ì²­ë…„ ìì˜ì—…ìì— ëŒ€í•´ì„  ì§€ì›ì„ íŠ¹í™”
Processed count: 8
Answer for question: ì¶©ë¶ì§€ë°©ì¤‘ì†Œê¸°ì—…ë²¤ì²˜ê¸°ì—…ì²­ì—ì„œ ë°±ë…„ê°€ê²Œë¥¼ ë½‘ì•„ ë•ëŠ” ì´ìœ ëŠ” ë­ì§€ : ì˜¤ëœ ê¸°ê°„ ë…¸í•˜ìš°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§€ì†ê°€
Processed count: 9
Answer for question: ì¶©ì²­ì§€ë°©í†µê³„ì²­ì— ì˜í•˜ë©´ ì¶©ë¶ì§€ì—­ì—ì„œ ì¼ ë…„ ë™ì•ˆ ë†’ì•„ì§„ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë¼ : 260
Processed count: 10
```
**F1 Score**
```
{'f1': 62.530103995621246}
```
**Contest Score**
```
0.77511
```








